dtype: jnp.bfloat16
softmax_dtype: jnp.float32
batch_size: 64
vocab_size: 2048
hidden_size: 1024
max_seq_length: 512
mlp_expansion: 4
dropout_rate: 0.1
num_heads: 4
head_dim: 128
causal_mask: True
remat: ["MLP", "Attn"] #"transformer"
scan_layers: True
num_transformer_layers: 12
num_outputs: 2048
learning_rate: 4e-4
num_minibatches: 4
seed: 42

