dtype: jnp.bfloat16
softmax_dtype: jnp.float32
vocab_size: 10000
hidden_size: 256
max_seq_length: 100
mlp_expansion: 2
dropout_rate: 0.1
num_heads: 4
head_dim: 256
causal_mask: True
remat: ["transformer", "MLP", "Attn"]
scan_layers: True
num_transformer_layers: 10
num_outputs: 10



input_size: 128
