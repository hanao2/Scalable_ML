# Efficient Training of a  Transformer Model on a Single GPU

Transformer is a large model, and onse should be familiar with how to train it using limited resources. In this project we develop a transformer model, and look into benefitting from the three training hacks (mixed precision, activation checkpointing and gradient accumulation) that we have discussed in a separate [repo](../single-gpu-training-hacks/).

We also try to profile this model, as a way to optimize its performance.





